= OpenShift AI Configuration

This chapter begins with running & configured OpenShift AI environment, if you don't already have your environment running, head over to Chapter 2. 

Lots to cover in section 1, we add the Ollama custom Runtime, Create a Data Science Project, Setup Storage, Create a Workbench, and finally serving the Ollama Framework, utilizing the Single Model Serving Platform to deliver our model to our Notebook Application. 


In section 2 we will explore using the Jupyter Notebook from our workbench, infere data from the Mistral 7B LLM.  While less technical than previous section of this hands on course, there are some steps download the Mistral Model, updating our notebook with inference endpoint, and evaluating our Models performance. 

Let's get started  ---