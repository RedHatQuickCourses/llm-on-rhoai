= OpenShift AI Configuration

This chapter begins with running and  configured OpenShift AI environment. If you don't already have your environment running, head over to Chapter 2. 

There's a lot to cover in section 1, we add the Ollama custom Runtime, create a data science project, setup storage, create a workbench, and finally serve the Ollama Framework, utilizing the Single Model Serving Platform to deliver our model to our Notebook Application. 


In section 2, we will explore using the Jupyter Notebook from our workbench to infer data from the Mistral 7B LLM. While less technical than previous section of this hands-on course, there are some steps to download the Mistral Model, update our notebook with inference endpoint, and evaluate our Models performance. 

Let's get started!