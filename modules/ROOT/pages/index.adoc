= Serving LLM Models on OpenShift AI
:navtitle: Home


video::intro_v4.mp4[width=800]

Welcome to this quick course on _Serving an LLM using OpenShift AI_. 

This program was designed to guide you through the process of installing an OpenShift AI Platform using an OpenShift Container Platform Web Console UI.  We get hands-on experience in each component needed to enable a RHOAI Platform using an Openshift Container Platform Cluster. 

Once we have an operational OpenShift AI Platform. We will login and begin the process of configuration of: Model Runtimes, Data Science Projects, Data connections, & finally use a jupyter notebook to infer the answers to easy questions. 

There will be some challenges along the way, all designed to teach us about a component, or give us the knowledge utilizing OpenShift AI and hosting an Large Language Model. 

If you're ready, letâ€™s get started !


IMPORTANT: The hands-on labs in this course were created and tested with RHOAI v2.9.1 & later. Labs will work without any changes in minor dot release upgrades of the product. Please open issues in this repository if you face any issue.


== Authors

The PTL team acknowledges the valuable contributions of the following Red Hat associates:

* Christopher Nuland

 * Vijay Chebolu

 * Noel O'Conner

 * Hunter Gerlach

 * Karlos Knox

== Classroom Environment

We will use the https://demo.redhat.com/catalog?item=babylon-catalog-prod%2Fopenshift-cnv.ocpmulti-wksp-cnv.prod[*Red Hat OpenShift Container Platform Cluster*] catalog item in the Red Hat Demo Platform (RHDP) to run the hands-on exercises in this course.

[TIP]
If you are planning on starting this course now, go ahead & launch the workshop now. It takes <10 minutes to provision, which is just enough time to finish the introduction section. 

When ordering this catalog item in RHDP:

  . Select Practice/Enablement for the Activity field

  . Select Learning about the Product for the Purpose field

  . Enter Learning RHOAI in the Salesforce ID field

  . Scroll to the bottom, and check the box to confirm acceptance of terms and conditions

  . Click order

For Red Hat partners who do not have access to RHDP, provision an environment using the Red Hat Hybrid Cloud Console. Unfortunately, the labs will NOT work on the trial sandbox environment. You need to provision an OpenShift AI cluster on-premises, or in the supported cloud environments by following the product documentation at https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.9/html/installing_and_uninstalling_openshift_ai_self-managed/index[Product Documentation for Red Hat OpenShift AI 2024].

== Prerequisites

 * Basic experience with Red Hat OpenShift Container Platform is recommended but is not mandatory.  

* We will encounter & modify code segments, deploy resources using YAML files, & modify launch configurations; but we will not have to write code.

== Objectives

The overall objectives of this course include:

 * Utilize Red Hat OpenShift AI to serve & interact with an LLM

 * Install Red Hat OpenShift AI operators & dependencies

 * Add a custom model serving runtime

 * Create a data science project, workbench & data connections

 * Load an LLM model into the Ollama runtime framework

 * Import (from git repositories), interact with LLM model via a Jupyter Notebooks

 * Experiment with the Mistral LLM