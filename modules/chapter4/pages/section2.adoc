= Mistral LLM Model Inference

video::llm_model_v3.mp4[width=640]

=== Create the Prompt

This cell sets the *system message* portion of the query to our model. Normally, we don't get the see this part of the query. This message details how the model should act, respond, and consider our questions. It adds checks to valdiate the information is best as possible, and to explain answers in detail.

== Memory for the conversation

This cell keeps track of the conversation, this way history of the chat are also sent along with new chat information, keeping the context for future questions.

The next cell tracks the conversation and prints it to the Notebook output window so we can experience the full conversation list. 

=== First input to our LLM 

The Notebooks first input to our model askes it to describe Paris in 100 words or less. 

In green text is the window, there is the setup message that is sent along with the single sentence question to desctibe to the model how to consider and respond to the question.

It takes approximately 12 seconds for the model to respond with the first word of the reply, and the final word is printed to the screen approximately 30 seconds after the request was started.

image::paris.png[width=800]

The responce answered the question in a well-considered and informated paragraph that is less than 100 words in length.

=== Second Input

Notice that the Second input - "Is there a River" - does not specify where the location is that might have a River. Because the conversation history is passed with the second input, there is not need to specify any additional informaiton.

image::london.png[width=800]

The total time to first word took approximately 14 seconds this time, just a bit longer due the orginal information being sent. The time for the entire reponse to be printed to the screen just took over 4 seoncds.

Overall our Model is performing well without a GPU and in a container limited to 4 cpus & 10Gb of memory.

== Second Example Prompt

Similar to the previous example, except we use the City of London, and run a cell to remove the verbose text reguarding what is sent or recieved apart from the answer from the model.

There is no change to memory setting, but go ahead and evalute where the second input; "Is there a river?" is answer correctly.

== Experimentation with Model 

Add a few new cells to the Notebook.

image::experiment.png[width=800]

Experiment with clearing the memory statement, then asking the river question again. Or perhaps copy one of the input statements and add your own question for the model. 

Try not clearing the memory and asking a few questions.

**You have successfully deployed a Large Language Model, now test the information that it has available and find out what is doesn't know.**


== Delete the Environment

Once you have finished experimenting with questions, make sure you head back to the Red Hat Demo Platform and delete the Openshift Container Platform Cluster.

You don't have to remove any of the resources; deleting the environment will remove any resources created during this lesson.

=== Leave Feedback

If you enjoyed this walkthrough, please send the team a note.
If you have suggestions to make it better or clarify a point, please send the team a note.

Until next time, Keep being Awesome!