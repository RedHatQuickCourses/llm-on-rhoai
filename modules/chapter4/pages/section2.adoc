= Mistral LLM Model Inference

video::llm_model_v3.mp4[width=640]

== Let's Talk with the LLM

=== First Input

The first input cell sent via the notebook to the Mistral model askes it to describe Paris in 100 words or less. 

In green text is the window, there is the setup message that is sent along with the single sentence question to desctibe to the model how to consider and respond to the question. This is known as the system message.  Additionally, is the current conversation which contains the Human Question or Prompt sent to Model, and next is the AI answer.

It takes few seconds for the OpenShift AI model to respond with the first words of the reply. The response answered the question in a well-considered informative paragraph that is less than 100 words in length. 

image::paris.png[width=800]


=== Second Input

Notice that the Second input - "Is there a River" - does not specify where the location is. Due to the conversation history being passed with the second input, there is no need to specify any additional informaiton. 

== Second Example 

Before we continue with London example, we execute a cell to  change the conversation mode to non-verbose. This elimiates the context of the prompt displayed in the notebook to instead just show the model's reply. 

We also execute a cell to clear memory, or the conversation history reguarding Paris. 

We did not disable the memory, or the verbosity of the conversation; we simply hid that section from being visible in the notebook.  

Go ahead run the second exmaple cells and evalute the responses from the Model.

image::london.png[width=800]

== Experimentation with Model 

There are multiple different types of large language models, while we can read about them, using them first hand is best way to experience how they perform. 

So now it's time to experiement on your own, or continue to follow along with this guide.


Add a few new cells to the bottom of the Notebook.

image::experiment.png[width=800]

Experiment by coping the clear memory cell text, paste the contents into one of the new cells.  Next copy one of the input statements and add your own question for the model.  Then run or execute those cells to learn more about the models capabilities.

I used the following examples:

 . Are you an AI model ?
 . Tell me a joke please ?

Then I asked one of my standard questions across models to determine it's knowledge of history: 

*Was George Washington Married?*

Why I ask ths question is several models say GW was married twice. I believed the first one, and this had me thinking several of the next models where wrong. It's critical that we evalute models to determine their viability for business use cases.

Try clearing the memory and asking your own questions.  

Continue to experiment with the Mistral model, or move to the next section, where we evaluate a differnet large language model. 