= Model Serving

Why Ollama -  It's unique value is that it makes installing and running LLMs very simple, even for non-technical users.  Reduces the resources requirement for many models by >50%, and also the dependency of a GPU with excellent performance in my opinion.


== Learn by Doing

In this quickcourse, there is one goal. Deploy an LLM in OpenShift AI, then utilize Jupyter Notebooks to query said LLM.   

Along the way, we'll discuss personas or roles that would perform these at our Customers. 

For example, you don't get to start with an OpenShift AI Platform, instead you will start with an OpenShift Container Cluster.  In the Section Two of this course, together we will tackle the challenges of upgrading our OCP Cluster to host our OpenShift AI Platform. 

Why should this matter to you,  it will provided an solid overview of the components needed, will allow you to explain the difficulty level of installing OpenShift AI, and will give you the experience to better understand what value each component adds to the Mix. 

There are several times that tasks must be performed in OCP related to the operations of the OPenShift AI Platform, so it's best to be familar with the both Dashboards.  

While the actuall installation can performed in a few minutes, there is an advanced setup that we need to perform to solve an issue with Cluster Security Certificates.   Most organizations will run their own TLS certifcates, rather than use the self-generated certificates in the cluster. 

[note:]
 The reason we need to perform this porition is that the Original OpenShift Container CLuster created Self Signed Certificates upon deployment.  When we install OpenShift AI, it will also create a set of certificates for deploying resources.  Well, when we expose resources externally, they use the OCP cluster certificates, which causes a mismatch when we try to connect remotely.  So instead of having two different sets of certificates, we are going to use the OCP cluster certificates for the OpenShift AI cluster, simipling connecting to the running model.

1. Once we complete the OpenShift AI setup, which should take about 15-20 minutes, the next step is to Launch OpenShift AI.  

We will then add the Ollama Model Serving Runtime .Yaml file as an additional Single Model Serving Option.  

1. Moving onto the next step, we will create our first Data Science Project in the OpenShift AI platform.  This will provide an isolated workspace for our resources.  

For Step 4 we need external storage, or remotely accessible storage via an API in order to retrive the LLM model file.   We will use MinIO for this purpose. We will deploy another .YAML file in the Data Science Project we just created.   

Next we will create storage buckets, update the file needed by the Ollama model to our new bucket in a sub-folder

Once that is complete we head back to our Project, and create a new workbench, will deploy or UI interface, which will be a jupyter notebook. 

Once that is complete, we can finally launch our Ollma Single Model Server. 

Then we will need to configuire the model to be hosted by our Ollama framework which will be Mistral 7B.

Once that is complete, we can add git repository to the Jupyter notebook and interact with our model using the LangChain library.

The last step is for you to interact with your new LLM model, hosted in OpenShift AI.  You can query the model with your questions, to determine how good it is.  

Then if you up to the Challenge - Delete the model, and redeploy the Ollama fRamework and deploy a different model, perhaps Llama2, or Lava and compare the performance of the different models.  You'll be on your own for this part, but I know you got this!


+
```shell
  pip install -r /opt/app-root/src/rhods-qc-apps/4.rhods-deploy/chapter1/requirements.txt
```
+
image::terminal-install.png[terminal install]

. Open the notebook **purchase-amount** from the **rhods-qc-apps/4.rhods-deploy/chapter1/purchase-amount.ipynb** directory:
+
image::purchase-amount-notebook.png[purchase-amount notebook]

. Run the notebook, and notice the creation of a new file in your environment, the `mymodel.pkl`
+
image::mymodel-pkl.png[Model file export]

[IMPORTANT]
====
There are different formats and libraries to export the model, in this case we are using pickle. Other common formats are:

* Protobuf

* MLeap

* H5

* ONNX

* PMML

* Torch

The use of either of those formats depends on the target server runtime, some of them are proven to be more efficient than others for certain type of training algorithms and model sizes.
====

=== Use the Model in Another Notebook

The model can be deserialized in another notebook, and used to generate a prediction:

. Open the notebook **use-purchase-amount** from the **rhods-qc-apps/4.rhods-deploy/chapter1/use-purchase-amount.ipynb** directory:
+
image::use-purchase-amount-notebook.png[use-purchase-amount notebook create]

. Run the **use-purchase-amount** notebook and notice the result:
+
- You can get the same result without training the model again.
- You are not training the model in the **use-purchase-amount** notebook, you are re-using the output from the training notebook, and using the generated model to generate an inference.

[TIP]
====
At this moment the model can be exported and imported in other projects for its use. Normally there will be an S3 bucket or a model registry to store models and versions of such models, and instead of manually exporting the model, there would be pipelines making the model available.
====

== Use the Model in a Container

For this section, you need Podman to create an image, and a registry to upload the resulting image.

=== Web application that uses the model

The pickle model that we previously exported can be used in a Flask application. In this section we present an example Flask application that uses the model.

[IMPORTANT]
====
Although we are actually serving a model with Flask in the exercise, Flask is not considered part of the Model Serving feature. This example represents one way in which some customers decide to embed their models in containers, although RHOAI provides for mechanisms that can make this process of serving a model a simpler process, when provided with the proper model formats.
====

. In your computer, create a new directory to save the source code of the web application.
Navigate to that directory.

. Download the `mymodel.pkl` file from JupyterLab into this directory.

. Open the directory with a python IDE, then create a python script named `app.py` with the following code:
+
```python[app.py]
from flask import Flask, request
import pickle

app = Flask(__name__)
# Load model
with open('mymodel.pkl', 'rb') as f:
    model = pickle.load(f)

model_name = "Time to purchase amount predictor"
model_file = 'model.plk'
version = "v1.0.0"


@app.route('/info', methods=['GET'])
def info():
    """Return model information, version how to call"""
    result = {}

    result["name"] = model_name
    result["version"] = version

    return result


@app.route('/health', methods=['GET'])
def health():
    """REturn service health"""
    return 'ok'


@app.route('/predict', methods=['POST'])
def predict():
    feature_dict = request.get_json()
    if not feature_dict:
        return {
            'error': 'Body is empty.'
        }, 500

    try:
        return {
            'status': 200, 
            'prediction': int(model(feature_dict['time']))
        }
    except ValueError as e:
        return {'error': str(e).split('\n')[-1].strip()}, 500


if __name__ == '__main__':
    app.run(host='0.0.0.0')
```

. Create a `requirements.txt` to describe the python dependencies to install on container startup:
+
```[requirements.txt]
click==8.0.3
cycler==0.11.0
Flask==2.0.2
fonttools==4.28.5
gunicorn==20.1.0
itsdangerous==2.0.1
Jinja2==3.0.3
kiwisolver==1.3.2
MarkupSafe==2.0.1
matplotlib==3.5.1
numpy==1.22.0
packaging==21.3
pandas==1.3.5
Pillow==9.0.0
pyparsing==3.0.6
python-dateutil==2.8.2
pytz==2021.3
scikit-learn==1.0.2
scipy==1.7.3
six==1.16.0
sklearn==0.0
threadpoolctl==3.0.0
Werkzeug==2.0.2
```

. Create a `Containerfile` to build an image with the Flask application:
+
```docker[containerfile]
# Base image
FROM python:3.9

# Set working directory
WORKDIR /app

# Copy files
COPY app.py /app <1>
COPY requirements.txt /app <2>
COPY mymodel.pkl /app <3>

# Install dependencies
RUN pip install -r requirements.txt

# Run the application
EXPOSE 8000
ENTRYPOINT ["gunicorn", "-b", "0.0.0.0:8000", "--access-logfile", "-", "--error-logfile", "-", "--timeout", "120"]
CMD ["app:app"]
```
<1> The python application source code
<2> The list of packages to install
<3> The model

. Build and push the image to an image registry
+

[source,console]
----
$ podman login quay.io
$ podman build -t purchase-predictor:1.0 .
$ podman tag purchase-predictor:1.0 quay.io/user_name/purchase-predictor:1.0
$ podman push quay.io/user_name/purchase-predictor:1.0
----
+
[NOTE]
====
If you are running macOS ARM versions, then run:

podman build --platform linux/amd64 -t purchase-predictor:1.0 .

====
+
After you push the image, open quay.io in your browser and make the image public.

. Deploy the model image to **OpenShift**. Get the OCP_CLUSTER_URL value from your RHDP page for this classroom.
+
[source,console]
----
$ oc login <OCP_CLUSTER_URL>:6443
$ oc new-project model-deploy
$ oc new-app --name purchase-predictor quay.io/user_name/purchase-predictor:1.0
$ oc expose service purchase-predictor
----

. Get the route for the deployed application
+
[source,console]
----
$ ROUTE_NAME=$(oc get route purchase-predictor -o jsonpath='{.spec.host}')
----

Now we can use the Flask application with some commands such as:
[source,console]
----
$ curl http://$ROUTE_NAME/health
ok
$ curl http://$ROUTE_NAME/info
{"name":"Time to purchase amount predictor","version":"v1.0.0"}
$ curl -d '{"time":4}' -H "Content-Type: application/json" \
> -X POST \
> http://$ROUTE_NAME/predict
{"prediction":34,"status":200}
----

[IMPORTANT]
====
In this section we have manually:

. Developed an application that uses the model

. Built an image with such application

. Push the image to a registry

. Deployed the containerized application in OpenShift

. Exposed the application's endpoint in OpenShift by creating a route

. Consumed the model through the application's REST API to request a prediction

There are automated and faster ways to perform these steps. In the following sections, we will learn about runtimes that only require you to provide a model, and they automatically provision an inference service for you.
====

== RHOAI Model Serving Runtimes

In the previous example, we manually created a Model Server by sending the model to an image that can interpret the model and expose it for consumption. In our example we used Flask.

However, in Red Hat OpenShift AI, you do not need to manually create serving runtimes.
By default, Red Hat OpenShift AI includes a pre-configured model serving runtime, OpenVINO, which can load, execute, and expose  models trained with TensorFlow and PyTorch.
OpenVINO supports various model formats, such as the following ones:

https://onnx.ai[ONNX]::
An open standard for machine learning interoperability.

https://docs.openvino.ai/latest/openvino_ir.html[OpenVino IR]::
The proprietary model format of OpenVINO, the model serving runtime used in OpenShift AI.

In order to leverage the benefits of OpenVINO, you must:

. Export the model in a format compatible with one of the available RHOAI runtimes.
. Upload the model to an S3
. Create a Data Connection to the S3 containing the model
. Create or use one of the available serving runtimes in a Model Server configuration that specifies the size and resources to use while setting up an inference engine.
. Start a model server instance to publish your model for consumption

While publishing this model server instance, the configurations will allow you to define how applications securely connect to your model server to request for predictions, and the resources that it can provide.

=== Model Serving Resources

When you use model serving, RHOAI uses the `ServingRuntime` and `InferenceService` custom resources.

ServingRuntime::
Defines a model server.

InferenceService::
Defines a model deployed in a model server.

For example, if you create a model server called `foo`, then RHOAI creates the following resources:

* `modelmesh-serving` Service
* `foo` ServingRuntime
** `modelmesh-serving-foo` Deployment
*** `modelmesh-serving-foo-...` ReplicaSet
**** `modelmesh-serving-foo-...-...` Pod

The `ServingRuntime` defines your model server and owns a `Deployment` that runs the server workload.
The name of this deployment is prefixed with the `modelmesh-serving-` prefix.
Initially, when no models are deployed, the deployment is scaled to zero, so no pod replicas are running.

When creating the first model server in a data science project, RHOAI also creates a `Service` called `modelmesh-serving` to map HTTP, HTTPs and gRPC traffic into the model servers.

[NOTE]
====
The `modelmesh-serving` service maps traffic for all model servers.
No additional services are created when you create more than one model server.
====

After you create a model server, you are ready to deploy models.
When you deploy a model in a model server, RHOAI creates an `InferenceService` custom resource, which defines the deployed model properties, such as the name and location of the model file.
For example, If you deploy a model called `my-model`, then RHOAI creates the following resources.

* `my-model` InferenceService
** `my-model` Route, which points to the `modelmesh-serving` Service.

[NOTE]
====
The route is only created if you have selected the `Make deployed models available through an external route` checkbox when creating the server.
The `InferenceService` owns the route.
====

At the same time, to be able to serve the model, RHOAI starts the model server by scaling the `model-serving-` deployment up to one pod replica.
This model serving pod runs the model serving containers:

* `mm`: the ModelMesh model serving framework.
* The model serving runtime container, such as `ovms` for OpenVINO.
* The ModelMesh https://github.com/kserve/modelmesh-runtime-adapter[runtime adapter] for your specifc serving runtime.
For example, if you are using OpenVINO, then the container is `ovms-adapter`.
* `rest-proxy`: For HTTP traffic.
* `oauth-proxy`: For authenticating HTTP requests.

[NOTE]
====
The `modelmesh-serving` pod runs the model server, which handles one or more deployed models.
No additional pods are created when you deploy multiple models.
====

