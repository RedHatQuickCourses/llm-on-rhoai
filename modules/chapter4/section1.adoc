= Model Serving

Why Ollama -  It's unique value is that it makes installing and running LLMs very simple, even for non-technical users.  Reduces the resources requirement for many models by >50%, and also the dependency of a GPU with excellent performance in my opinion.


== Learn by Doing

In this quickcourse, there is one goal. Deploy an LLM in OpenShift AI, then utilize Jupyter Notebooks to query said LLM.   

Along the way, we'll discuss personas or roles that would perform these at our Customers. 

For example, you don't get to start with an OpenShift AI Platform, instead you will start with an OpenShift Container Cluster.  In the Section Two of this course, together we will tackle the challenges of upgrading our OCP Cluster to host our OpenShift AI Platform. 

Why should this matter to you,  it will provided an solid overview of the components needed, will allow you to explain the difficulty level of installing OpenShift AI, and will give you the experience to better understand what value each component adds to the Mix. 

There are several times that tasks must be performed in OCP related to the operations of the OPenShift AI Platform, so it's best to be familar with the both Dashboards.  

While the actuall installation can performed in a few minutes, there is an advanced setup that we need to perform to solve an issue with Cluster Security Certificates.   Most organizations will run their own TLS certifcates, rather than use the self-generated certificates in the cluster. 


....
 The reason we need to perform this porition is that the Original OpenShift Container CLuster created Self Signed Certificates upon deployment.  When we install OpenShift AI, it will also create a set of certificates for deploying resources.  Well, when we expose resources externally, they use the OCP cluster certificates, which causes a mismatch when we try to connect remotely.  So instead of having two different sets of certificates, we are going to use the OCP cluster certificates for the OpenShift AI cluster, simipling connecting to the running model.
....



1. Once we complete the OpenShift AI setup, which should take about 15-20 minutes, the next step is to Launch OpenShift AI.  

We will then add the Ollama Model Serving Runtime .Yaml file as an additional Single Model Serving Option.  

1. Moving onto the next step, we will create our first Data Science Project in the OpenShift AI platform.  This will provide an isolated workspace for our resources.  

For Step 4 we need external storage, or remotely accessible storage via an API in order to retrive the LLM model file.   We will use MinIO for this purpose. We will deploy another .YAML file in the Data Science Project we just created.   

Next we will create storage buckets, update the file needed by the Ollama model to our new bucket in a sub-folder

Once that is complete we head back to our Project, and create a new workbench, will deploy or UI interface, which will be a jupyter notebook. 

Once that is complete, we can finally launch our Ollma Single Model Server. 

Then we will need to configuire the model to be hosted by our Ollama framework which will be Mistral 7B.

Once that is complete, we can add git repository to the Jupyter notebook and interact with our model using the LangChain library.

The last step is for you to interact with your new LLM model, hosted in OpenShift AI.  You can query the model with your questions, to determine how good it is.  

Then if you up to the Challenge - Delete the model, and redeploy the Ollama fRamework and deploy a different model, perhaps Llama2, or Lava and compare the performance of the different models.  You'll be on your own for this part, but I know you got this!



